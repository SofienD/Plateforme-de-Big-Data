______________________________________________________________________________________________________________________________________________
*****************************************Plateforme du Big Data Hadoop Checkpoint*************************************************************
______________________________________________________________________________________________________________________________________________


******Hive***********
---------------------------------------------------------------------------------------------------
1. Create a book table where the schema is as follows: : (id INT, title STRING, publishDate STRING):
---------------------------------------------------------------------------------------------------
$ cd $PLAY_AREA
$ hive
CREATE TABLE books (id INT, title STRING, publishDate STRING)
ROW FORMAT DELIMITED
FIELDS TERMINATED BY ','
STORED AS TEXTFILE;
-------------------------------------------------------------------------------------------
2. import data from the file /checkpoint/books/books.txt (stored locally) in the book table.
-------------------------------------------------------------------------------------------
LOAD DATA LOCAL INPATH 'data/books.txt'
OVERWRITE INTO TABLE books;
----------------------------------------
3. display the records where the id is 2
----------------------------------------
select * from books where id=2;
--------------------------------------------------------------------------------------
4. Create a new sales table in the following HDFS location:‘/user/cloudera/checkpoint’
Thesales table contains 3 columns : (id INT, buyer STRING, purchaseDate STRING) 
--------------------------------------------------------------------------------------
CREATE EXTERNAL TABLE sales
(id INT, buyer STRING, purchaseDate STRING)
ROW FORMAT DELIMITED
FIELDS TERMINATED BY '\t'
STORED AS TEXTFILE
LOCATION '‘/user/cloudera/checkpoint';
--------------------------------------
5. Display 5 rows of the sales table.
--------------------------------------
select * from sales limit 5;
-----------------------------------------------------------------------------------------------------------------
6. Create the table book_sales with the following fields:(id INT, title STRING, buyer STRING, purchaseDate STRING)
-----------------------------------------------------------------------------------------------------------------
CREATE TABLE books_sales (id INT, title STRING, buyer STRING, purchaseDate
STRING)
ROW FORMAT DELIMITED
FIELDS TERMINATED BY ','
STORED AS TEXTFILE;
--------------------------------------------------------------------------------------------------
7. Fill the book-sales table using a joint according to column id among the tables: books and sale.
--------------------------------------------------------------------------------------------------
INSERT OVERWRITE TABLE books_sales
SELECT b.id, b.title, p.buyer, p.purchaseDate
FROM books b JOIN purchases p ON (b.id=p.id);
-------------------------------------------
8. Show 10 records of the table book_sale.
-------------------------------------------
 select * from books_sale limit 10;
--------------------------------------------
9. Delete book tables, sales, and book_sales.
---------------------------------------------
drop table books_sales;
drop table sales;
drop table books;
-------------------------------------------------(PIG)---------------------------------------------
1-Copy the two files ‘artists.json’ and ‘movies.json’ under HDFS.
----------------------------------------------------------------------------------------------------

{ "_id": "movie:1", "title": "Vertigo", "year": 1958, "genre":
"drama", "summary": "...", "country": "USA", "director": { "_id": "artist:3"},
 "actors": [{ "_id": "artist:15", "role": "John
             Ferguson" }, { "_id": "artist:16", "role": "Madeleine Elster"
     }]
 }

{ "_id": "artist:15", "last_name": "Stewart", "first_name":
"James", "birth_date": "1908" }
-----------------------------------------
2-Load the two files using JsonLoader
----------------------------------------
artists = LOAD 'artists-pig.json'
      USING JsonLoader('id: chararray, firstName:chararray,
         lastName:chararray, birth:chararray');

movies = LOAD 'movies-pig.json'
  USING JsonLoader('id:chararray, title:chararray, year:chararray,
                    genre:chararray, summary:chararray, country:chararray,
                    director: (id:chararray,lastName:chararray,
                           firstName:chararray,birthDate:chararray),
                    actors: {(id:chararray, role:chararray)}'
                        );
--------------------------------------------------------------------------------------
3. Display the list of American films by year (show the year and the names of movies)
---------------------------------------------------------------------------------------
moviesUSA = filter movies BY country=='US';
mUSA_group = group moviesUSA by year;
mUSA_year = foreach mUSA_group generate group, moviesUSA.title;
---------------------------------------------------
4- Display each director’s list of American films
---------------------------------------------------
mUSA_group2 = group moviesUSA by director;
mUSA_director = foreach mUSA_group2 generate group, moviesUSA.title;
----------------------------------------------------
5. Display the triplets (idFilm, idActeur, role)
----------------------------------------------------
mUSA_actors = foreach movies generate id, flatten(actors);
--------------------------------------------------------------------
6-. Display for each film the complete descriptions of its actors
-------------------------------------------------------------------
moviesActors = join mUSA_actors by actors::id, artists by id;
----------------------------------------------
7- Show for each film the number of its actors
----------------------------------------------
moviesActors = join mUSA_actors by actors::id